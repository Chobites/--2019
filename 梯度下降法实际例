#梯度下降
import numpy as np
import matplotlib.pyplot as plt
#----------------------------------
data = np.genfromtxt("data.csv",delimiter=',')
x_data = data[:,0]
y_data = data[:,1]
plt.scatter(x_data,y_data)
plt.show()
#--------------------------------
#学习率
learning_rate = lr = 0.001
#截距
b = 0
#斜率
k = 0
#最大迭代次数
epochs = 50

#最小二乘法
def compute_error(b,k,x_data,y_data):
    totalError = 0
    for i in range(0,len(x_data)):
        totalError += (y_data[i] - (k*x_data[i]+b))**2
    return totalError/float(len(x_data))
def gradient_decent_runner(x_data,y_data,b.k.lr,epochs):
    #计算总数据量
    m = float(len(x_data))
    #循环epochs次
    for i in range(epochs):
        b_grad = 0
        k_grad = 0
        #计算梯度的综合再求平均
        for j in range(0,len(x_data)):
            b_grad += -(1/m) * (y_data[i] - (k * x_data[i] + b))
            k_grad += -(1/m) * (y_data[j] - ((k * x_data[j]  + b))
        #更新世界的锋芒
        b = b - (lr * b_grad)
        k = k - (lr * k_grad)
        #
        if i % b ==0:
            print("epochs:",i)
            plt.plot(x_data,y_data,'b')
            plt.plot(x_data,k*y_data+b,'r')
            plt.show()
    return b,k
    #-----------------------------
print("开始是 b= {0},k={0},error={2}".format(b,k,compute_error(b,k,x)data,y_data))
print('Running.......')
b,k = gradient_decent_runner(x_data,y_data,b,k,lr,epochs)
print('After {0} iterations b = {},k = {},error= {}'.format(epochs,b,k))
